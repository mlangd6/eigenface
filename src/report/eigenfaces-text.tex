\documentclass[12pt,french]{article}

\usepackage[T1]{fontenc}
\usepackage[french]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}

% quelques définitions
\theoremstyle{plain}
\newtheorem{thm}{Théorème}
\newtheorem{cor}[thm]{Corollaire}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{dem}{Démonstration}

\theoremstyle{definition}
\newtheorem{defi}{Définition}
\newtheorem{rmq}{Remarque}
\newtheorem{ex}{Exemple}
\newtheorem{exo}{exercice}

\title{Eigenfaces}
\author{
  Bouarah Romain \and
  Langdorph Matthieu \and
  Ketels Lucas \and
  Souffan Nathan
}


\begin{document}
\maketitle
\newpage

\part{Partie Mathématiques}


\section{Calcul des Eigenfaces}

\subsection{Travail dans $\mathbb{R}^{N \times N}$}
Considérons une image de visage comme une matrice $N \times N$ dont le coefficient $(i,j)$ est égal au niveau de gris du pixel $(i,j)$ (l'origine se situant dans le coin haut gauche).
On transforme ensuite cette matrice comme un vecteur de $\mathbb{R}^{N \times N}$ en juxtaposant les colonnes l'une en dessous de l'autre, par exemple.
\[
  \begin{pmatrix}
    p_{1,1} & p_{1,2} & \cdots & p_{1,N} \\
    p_{2,1} & p_{2,2} & \cdots & p_{2,N} \\
    \vdots  & \vdots  & \ddots & \vdots  \\
    p_{N,1} & p_{N,2} & \cdots & p_{N,N}
  \end{pmatrix}
  \rightarrow
  \begin{pmatrix}
    p_{1,1} \\
    p_{2,1} \\
    \vdots \\
    p_{N,1} \\
    \vdots \\
    p_{1,N} \\
    \vdots \\
    p_{N,N}
  \end{pmatrix}
\]

\subsection{Calcul des valeurs propres et des vecteurs propres de la matrice de covariance}
Les images des visages sont globalement similaires, donc ces images ne seront pas distribuées aléatoirement dans notre espace $\mathbb{R}^{N \times N}$.
On peut donc décrire notre espace des visages de manière plus fine (\textit{i.e.} avec moins de dimensions).


\subsubsection{Matrice de covariance}
Avant de commencer la réduction de l'espace de travail, nous allons d'abord analyser cette dispersion en calculant la matrice de covariance.

\begin{defi}
  La matrice de covariance d'un vecteur de $p$ variables aléatoires $\overrightarrow{X} =
  \begin{pmatrix}
    X_1 \\
    \vdots \\
    X_p
  \end{pmatrix}$ dont chacune possède une variance, est la matrice carrée dont le terme générique est donné par $a_{i,j} = Cov(X_i,X_j)$.
\end{defi}
\begin{defi}
  La matrice de covariance, notée parfois $\Sigma$, est définie par $Var(\overrightarrow{X}) = \mathrm{E}[ (\overrightarrow{X}-\mathrm{E}(\overrightarrow{X})) (\overrightarrow{X}-\mathrm{E}(\overrightarrow{X}))^T]$
\end{defi}

Supposons que nous avons $M$ images de visage. On note $I = [I_1~I_2~\dots~I_M]$ la matrice de taille $N^2 \times M$ de l'ensemble de nos images. Normalisons nos images de visages :
\begin{enumerate}
\item On calcule le visage moyen $\Psi = \frac{1}{M}\displaystyle\sum_{i=1}^{M} I_i$.
\item On retire le visage moyen à chacun de nos visages, en effet nous nous intéressons uniquement aux particularités.
\end{enumerate}
Donc, chaque visage différe de la moyenne par le vecteur $\Phi_i = I_i - \Psi$. On pose $A = [\Phi_1~\Phi_2~\dots~\Phi_M]$ la matrice $N^2 \times M$ des visages normalisées.
On calcule alors la matrice de covariance $C = \frac{1}{M}AA^T$.

Cette matrice encode la dispersion de nos images de visages dans $\mathbb{R}^{N^2}$. Les coefficients de la diagonale sont les variances selon les axes $e_1, e_2, \dots, e_{N^2}$.
Les autres coefficients sont la covariance entre deux axes.

\begin{rmq}
  $C$ est symétrique réelle donc diagonalisable dans une base orthonormée. $C$ est également définie semi-postivie, c'est à dire que toutes ses valeurs propres sont positives
\end{rmq}

\subsubsection{Méthode 1: Analyse en composantes principales}
L'analyse en composantes principales consiste à transformer des variables liées entre elles (dites ``corrélées'' en statistique) en nouvelles variables décorrélées les unes des autres.

Ces nouvelles variables sont nommées ``composantes principales'', ou axes principaux, dans notre cas elles sont appelées \emph{eigenfaces}.

L'ACP nous permet de réduire le nombre de variables et de rendre l'information moins redondante. En effet, nous cherchons les meilleurs axes ou \emph{eigenfaces} décrivant le mieux notre espace de visages.

De plus, le nombre de \emph{eigenfaces} est toujours inférieurs au nombre d'images de visages.\\

Finalement, nous cherchons le vecteur $u \in \mathbb{R}^{N^2}$ tel que la projection des images des visages sur $u$ ait une variance maximale. Cette projection s'écrit :
\[
  p_u(A)=Au
\]
La variance empirique de $p_u(A)$ vaut donc $p_u(A)^T p_u(A) = u^T C u$

Comme C est symétrique réelle, elle est donc diagonalisable dans une base orthonormée.\\
Notons $P$ le changement de base associé et $D = Diag(\lambda_1, \dots, \lambda_L)$ la matrice diagonale formée de son spectre rangé en ordre décroissant, on a :
\[
  p_u(A)^T p_u(A) = u^T P^T D P u = (Pu)^T D \underbrace{(Pu)}_v
\]

Le vecteur unitaire $u$ qui maximise $v^T D v$  est un vecteur propre de $C$ associé à la valeur propre $\lambda_1$, on a alors :
\[
  v^T D v = \lambda_1
\]
La valeur propre $\lambda_1$ est la variance empirique sur le premier axe de l'ACP.
On continue la recherche du deuxième axe de projection $w$ sur le même principe en imposant qu'il soit orthogonal à $u$.
\subsubsection{Méthode 2: Décomposition en valeurs singulières}

La décomposition en valeurs singulières permet de factoriser des matrices carrés ou rectangulaires réels ou complexes, on s'intéressera ici au cas réels. \\
Énoncé: Soit $M$ une matrice $m \times n$, alors il existe une décomposition de la forme: \\
$$M=U\Sigma V^t$$
Avec $U$ une matrice unitaire $m \times m$, $\Sigma$ une matrice $m\times n$ où les coefficients diagonaux sont des réels positifs ou nuls et tous les autres sont nuls, et $V$ est une matrice unitaire $n \times n$. On appelle ainsi cette factorisation la décomposition en valeurs singulières de $M$.
\begin{itemize}
  \item La matrice $V$ contient un ensemble de vecteurs de base orthonormés de $\mathbb{R}^n$ d'entrée
  \item La matrice $U$ contient un ensemble de vecteurs de base orthonormés de $\mathbb{R}^m$ de sortie
  \item La matrice $\Sigma$ contient dans ses coefficients diagonaux les valeurs singulières de la matrice $M$. Elles correspondent aux racines des valeurs propres de $M^t M$
\end{itemize}

On appelle ainsi valeur singulière de $M$ toute racine carrée d'une valeur propre de $M^t M$, autrement dit tout réel positif $\lambda$ tel qu'il existe un vecteur unitaire $u$ dans $\mathbb{R}^m$ et un vecteur unitaire $v$ vérifiant dans $\mathbb{R}^n$:
$$ M^t u = \lambda v \text{ et } Mv = \lambda u$$

Dans le cas d'une matrice carrée symétrique définie semi-positive (ce qui est le cas ici pour $C$) les valeurs singulières et vecteurs singuliers correspondent aux valeurs propres et vecteurs propres de M.


Il existe plusieurs façon de calculer une décomposition en valeurs singulière. Un algorithme courant consiste en:
\begin{itemize}
  \item Effectuer une décomposition QR si la matrice possède plus de lignes que de colonnees
  \item Réduire le facteur R sous forme bidiagonale, (on pourra notaement utiliser des transformations de Householder alternativement sur les colonnes et sur les lignes de la matrice).
  \item Les valeurs singulières et vecteurs singuliers sont alors trouvés en effectuant une itération de type QR bidiagonale avec la procédure DBDSQR
\end{itemize}

\section{Utilisation des eigenfaces pour classer une image de visage}
% Les visages calculés avec les vecteurs propres forment un ensemble à l'aide duquel nous pouvons décrire un visage. \\
% 40 eigenfaces suffisent pour avoir une bonne description de l'ensemble des visages
\emph{Définir $M'$ comme étant le nombre de visage du training set}\\
\emph{Définir la base de l'espaces des eigenfaces}

\subsection{Projection dans l'espace des visages}
Soit $\Gamma$ une nouvelle image de visage, on la projette dans l'espace des visages par:
$$\omega_k = u_k^T(\Gamma - \Psi)$$
pour $k = 1,~\dots,~M'$, on obtient ainsi un vecteur $\Omega$ tel que:
\[\Omega =
  \begin{pmatrix}
    \omega_1 \\
    \vdots \\
    \omega_{M'}
  \end{pmatrix}
\]
$\Omega$ décrit la contribution de chacun des eigenfaces pour l'image en question.

\subsection{Analyse de la projection}
On peut maintenant utiliser ce vecteur pour reconnaître si ce vecteur correspond à un visage déjà connue étant dans la base ou si c'est un visage inconnu.
Pour ce faire, on cherche la classe $k$ qui minimise la distance euclidienne $\epsilon_k = \|(\Omega - \Omega_k)\|^2$
où $\Omega_k$ est le vecteur décrivant la $k^{ieme}$ classe de visage. Les classes de visages $\Omega_i$ sont calculés en faisant la moyenne de plusieurs ou une image du visage de chaque individu. \\
On considère ensuite qu'un visage appartient à une certaine classe de visage $k$ si le minimum $\epsilon_k$ est en dessous un certain seuil $\Theta$.
Si $\forall k,~\epsilon_k > \Theta$, alors le visage est inconnu et on a alors une nouvelle classe de visage. \\
Il y a finalement 4 possibilités pour une image:
\begin{enumerate}
\item L'image est proche de l'espace des visages et proche d'une classe de visage en particulier, c'est alors un visage connu
\item L'image est proche de l'espace des visages mais n'est proche d'aucune classe de visage, c'est alors un visage inconnu
\item L'image est distante de l'espace des visages mais proche d'une classe de visage, \emph{EXPLICATION!!!} C'est la plupart du temps un faux positif.
\item L'image est distante de l'espace des images et également distante de toutes les classes de visages, on peut alors en conlcure que ce n'est pas un visage
\end{enumerate}











\end{document}
